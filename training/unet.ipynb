{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.316912800Z",
     "start_time": "2025-03-14T17:34:55.249455800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 1000 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 8\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0.5\n",
    "\n",
    "# Training schedule\n",
    "lr = 1e-5\n",
    "l2 = 5e-1\n",
    "max_epochs=200\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 64 if is_local else int(sys.argv[-1])\n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'\n",
    "#channels_used = np.s_[...,[0,10,20,30]] # used only when r_method = 'fixed'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.332908300Z",
     "start_time": "2025-03-14T17:34:55.321048700Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.372527900Z",
     "start_time": "2025-03-14T17:34:55.337070200Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 130\n",
      "\tVAL: 51\n",
      "\tTEST: 47\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.373628700Z",
     "start_time": "2025-03-14T17:34:55.368436Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_annot_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use, augment=False):\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.channels_use = channels_use\n",
    "        self.augment=augment\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "        self.annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hdf5_filepaths)\n",
    "    \n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        # open hdf5 file\n",
    "        hdf5_file = h5py.File(self.hdf5_filepaths[idx],'r')\n",
    "        \n",
    "        # get mask\n",
    "        mask = torch.from_numpy(\n",
    "            hdf5_file['mask'][:],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # get ftir\n",
    "        ftir = torch.from_numpy(\n",
    "            hdf5_file['spectra'][*self.channels_use],\n",
    "        ).permute(2,0,1)\n",
    "        hdf5_file.close()\n",
    "        ftir *= mask\n",
    "        \n",
    "        # get annotations\n",
    "        annotations = self.split_annotations(cv2.imread(self.annotation_filepaths[idx])[:,:,::-1])\n",
    "        annotations *= mask\n",
    "        has_annotations = annotations.sum(dim=0) != 0\n",
    "        \n",
    "        if self.augment:\n",
    "            to_aug = torch.rand((2,))\n",
    "            if to_aug[0] > 0.5: #hflip\n",
    "                ftir = torch.flip(ftir, (-1,))\n",
    "                annotations = torch.flip(annotations, (-1,))\n",
    "                has_annotations = torch.flip(has_annotations, (-1,))\n",
    "                mask = torch.flip(mask, (-1,))\n",
    "            if to_aug[1] > 0.5: #vflip\n",
    "                ftir = torch.flip(ftir, (-2,))\n",
    "                annotations = torch.flip(annotations, (-2,))\n",
    "                has_annotations = torch.flip(has_annotations, (-2,))\n",
    "                mask = torch.flip(mask, (-2,))\n",
    "        \n",
    "        return ftir, annotations, mask, has_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.386012400Z",
     "start_time": "2025-03-14T17:34:55.375823200Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 65\n",
      "\tval: 26\n",
      "\ttest: 24\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used, augment=False,\n",
    ")\n",
    "dataset_test = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used, augment=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=False,drop_last=False)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.386012400Z",
     "start_time": "2025-03-14T17:34:55.381020700Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abdcc80122d23428"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1)\n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module):\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data, annotations, mask, has_annotations in train_loader:\n",
    "        where = torch.where(has_annotations[0] == 1)\n",
    "        ridxs = torch.randperm(where[0].shape[0])[:100]\n",
    "        spectral_sample.append(data[:, :, where[0][ridxs],where[1][ridxs]].permute(0,2,1).flatten(0,1).numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.419831100Z",
     "start_time": "2025-03-14T17:34:55.388496200Z"
    }
   },
   "id": "e49ecda26ba39e97",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        self.print = PrintLayer()\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x) \n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.419831100Z",
     "start_time": "2025-03-14T17:34:55.390040800Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet model with 31.137M params\n"
     ]
    }
   ],
   "source": [
    "if r_method == 'pca':\n",
    "    input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "elif r_method == 'fixed':\n",
    "    input_processing = FixedReduction(input_dim=len(wavenumbers_used))\n",
    "elif r_method == 'linear':\n",
    "    input_processing = LinearReduction(input_dim=len(wavenumbers_used),reduce_dim=reduce_dim)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    input_processing,\n",
    "    UNet(n_channels=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,n_classes=n_classes),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"unet model with {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M params\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.491251200Z",
     "start_time": "2025-03-14T17:34:55.400722800Z"
    }
   },
   "id": "ed359ba87a31e556",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=15, threshold=0.01, cooldown=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.545451Z",
     "start_time": "2025-03-14T17:34:55.487098900Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:34:55.553491800Z",
     "start_time": "2025-03-14T17:34:55.547771700Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ✰ ✰ ✰ EPOCH 1 ✰ ✰ ✰ \n",
      "train : █\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(out,annot\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m has_annotations \u001B[38;5;66;03m# loss per pixel\u001B[39;00m\n\u001B[0;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m/\u001B[39m (has_annotations\u001B[38;5;241m.\u001B[39msum()) \u001B[38;5;66;03m# mean loss per annotated pixel\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# backprop\u001B[39;00m\n\u001B[0;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Calculate metrics\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    583\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[0;32m    348\u001B[0m     tensors,\n\u001B[0;32m    349\u001B[0m     grad_tensors_,\n\u001B[0;32m    350\u001B[0m     retain_graph,\n\u001B[0;32m    351\u001B[0m     create_graph,\n\u001B[0;32m    352\u001B[0m     inputs,\n\u001B[0;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    355\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\n ✰ ✰ ✰ EPOCH {epoch+1} ✰ ✰ ✰ \")\n",
    "    \n",
    "    # reset running metrics\n",
    "    running_loss_train, running_loss_val = 0, 0\n",
    "    train_preds,train_targets = [],[]\n",
    "    val_preds,val_targets = [],[]\n",
    "    \n",
    "    # Train loop\n",
    "    model.train()\n",
    "    batch_frac = 42 / (len(train_loader))\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(train_loader):\n",
    "        print(f\"train : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "        \n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_train += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        train_preds.extend(preds.detach().cpu().numpy())\n",
    "        train_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"train : {'█'*42}\")\n",
    "        \n",
    "    # Validate loop\n",
    "    model.eval()\n",
    "    batch_frac = 42 / len(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, annot, mask, has_annotations) in enumerate(val_loader):\n",
    "            print(f\"val   : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "            \n",
    "            # Put data and label on device\n",
    "            data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "            \n",
    "            # Push data through model\n",
    "            out = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "            loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "            \n",
    "            # Calculate metrics\n",
    "            running_loss_val += loss.cpu().item()\n",
    "            targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "            preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "            val_preds.extend(preds.detach().cpu().numpy())\n",
    "            val_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"val   : {'█'*42}\")\n",
    "    \n",
    "    # calculate epoch metrics for train set\n",
    "    train_acc = accuracy_score(train_targets, train_preds); training_accs.append(train_acc)\n",
    "    train_f1m = f1_score(train_targets, train_preds, average='macro'); training_f1ms.append(train_f1m)\n",
    "    train_f1 = f1_score(train_targets, train_preds, average=None); training_f1s.append(train_f1)\n",
    "    train_loss = running_loss_train / (len(dataset_train)); training_losses.append(train_loss)\n",
    "    \n",
    "    # calculate epoch metrics for val set\n",
    "    val_acc = accuracy_score(val_targets, val_preds); validation_accs.append(val_acc)\n",
    "    val_f1m = f1_score(val_targets, val_preds, average='macro'); validation_f1ms.append(val_f1m)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average=None); validation_f1s.append(val_f1)\n",
    "    val_loss = running_loss_val / (len(dataset_val)); validation_losses.append(val_loss)\n",
    "    \n",
    "    # Update\n",
    "    print(f\"TRAIN --- | Loss: {train_loss:.4} | OA: {train_acc:.4} | f1: {train_f1m:.4}\")\n",
    "    print(f\"VAL ----- | Loss: {val_loss:.4} | OA: {val_acc:.4} | f1: {val_f1m:.4}\")\n",
    "    \n",
    "    scheduler.step(val_f1m)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != lr:\n",
    "        print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "        lr = new_lr\n",
    "        lr_decreases.append(epoch)\n",
    "        if len(lr_decreases) >= 3: \n",
    "            print(\"Val f1 decreased thrice, ending training early\")\n",
    "            break\n",
    "\n",
    "    if val_f1m > best_val_f1:\n",
    "        best_val_f1 = val_f1m\n",
    "        best_val_epoch = epoch\n",
    "        if not is_local:\n",
    "            torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:05.398477500Z",
     "start_time": "2025-03-14T17:34:55.555611200Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test\n",
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(test_loader):\n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        test_preds.extend(preds.detach().cpu().numpy())\n",
    "        test_targets.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "# calculate test set metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4} | OA: {test_acc:.4} | f1: {test_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:05.400548600Z",
     "start_time": "2025-03-14T17:35:05.399514800Z"
    }
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(1,len(training_losses)+1),np.array(training_losses),color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(1,len(validation_losses)+1),np.array(validation_losses),color='orange',label=\"validation\")\n",
    "ax[0].scatter(len(validation_losses),test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"loss curves\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(1,len(training_accs)+1),np.array(training_accs),color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(1,len(validation_accs)+1),np.array(validation_accs),color='orange',label=\"validation\")\n",
    "ax[1].scatter(len(validation_losses),test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(1,len(training_f1ms)+1),np.array(training_f1ms),color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(1,len(validation_f1ms)+1),np.array(validation_f1ms),color='orange',label=\"validation\")\n",
    "ax[2].scatter(len(validation_losses),test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"macro f1\"); ax[2].legend()\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "\n",
    "ax[0].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-14T17:35:05.400548600Z"
    }
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(6):\n",
    "    ax[cls].plot(np.arange(1,len(training_f1s)+1),[i[cls] for i in training_f1s], color='black', label=\"train\")\n",
    "    ax[cls].plot(np.arange(1,len(validation_f1s)+1),[i[cls] for i in validation_f1s], color=annotation_class_colors[cls]/255, label=\"val\")\n",
    "    ax[cls].set_title(f\"{annotation_class_names[cls]}\")\n",
    "    ax[cls].legend()\n",
    "    ax[cls].scatter(len(validation_losses),test_f1[cls],color='green',label=\"test\",marker=\"x\")\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[cls].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_by_class_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:05.401580600Z",
     "start_time": "2025-03-14T17:35:05.401580600Z"
    }
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:05.403642800Z",
     "start_time": "2025-03-14T17:35:05.402612300Z"
    }
   },
   "id": "b8248f10250d5d36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-14T17:35:05.402612300Z"
    }
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
