{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.639125500Z",
     "start_time": "2025-03-14T17:35:46.249425900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3fa5c69dc9bda7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 1000 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 2\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0.5\n",
    "\n",
    "# Training schedule\n",
    "lr = 1e-5\n",
    "l2 = 5e-1\n",
    "max_epochs=200\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'}\n",
    "reduce_dim = 64 if is_local else int(sys.argv[-1])\n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'\n",
    "#channels_used = np.s_[...,[0,10,20,30]] # used only when r_method = 'fixed'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.644442900Z",
     "start_time": "2025-03-14T17:35:49.634948800Z"
    }
   },
   "id": "222959fcfa561aff",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.676172600Z",
     "start_time": "2025-03-14T17:35:49.644442900Z"
    }
   },
   "id": "a78af96389a4cd31",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Datasets, Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaba53c2e93ca461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 130\n",
      "\tVAL: 51\n",
      "\tTEST: 47\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.685172Z",
     "start_time": "2025-03-14T17:35:49.677213100Z"
    }
   },
   "id": "e4655cf38851b265",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_annot_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use, augment=False):\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.channels_use = channels_use\n",
    "        self.augment=augment\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "        self.annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hdf5_filepaths)\n",
    "    \n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        # open hdf5 file\n",
    "        hdf5_file = h5py.File(self.hdf5_filepaths[idx],'r')\n",
    "        \n",
    "        # get mask\n",
    "        mask = torch.from_numpy(\n",
    "            hdf5_file['mask'][:],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # get ftir\n",
    "        ftir = torch.from_numpy(\n",
    "            hdf5_file['spectra'][*self.channels_use],\n",
    "        ).permute(2,0,1)\n",
    "        hdf5_file.close()\n",
    "        ftir *= mask\n",
    "        \n",
    "        # get annotations\n",
    "        annotations = self.split_annotations(cv2.imread(self.annotation_filepaths[idx])[:,:,::-1])\n",
    "        annotations *= mask\n",
    "        has_annotations = annotations.sum(dim=0) != 0\n",
    "        \n",
    "        if self.augment:\n",
    "            to_aug = torch.rand((2,))\n",
    "            if to_aug[0] > 0.5: #hflip\n",
    "                ftir = torch.flip(ftir, (-1,))\n",
    "                annotations = torch.flip(annotations, (-1,))\n",
    "                has_annotations = torch.flip(has_annotations, (-1,))\n",
    "                mask = torch.flip(mask, (-1,))\n",
    "            if to_aug[1] > 0.5: #vflip\n",
    "                ftir = torch.flip(ftir, (-2,))\n",
    "                annotations = torch.flip(annotations, (-2,))\n",
    "                has_annotations = torch.flip(has_annotations, (-2,))\n",
    "                mask = torch.flip(mask, (-2,))\n",
    "        \n",
    "        return ftir, annotations, mask, has_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.693903200Z",
     "start_time": "2025-03-14T17:35:49.686205200Z"
    }
   },
   "id": "a8a3aa59fbf57012",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader sizes:\n",
      "\ttrain: 65\n",
      "\tval: 26\n",
      "\ttest: 24\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used, augment=use_augmentation,\n",
    ")\n",
    "dataset_val = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used, augment=False,\n",
    ")\n",
    "dataset_test = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used, augment=False,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=False,drop_last=False)\n",
    "print(f\"loader sizes:\\n\\ttrain: {len(train_loader)}\\n\\tval: {len(val_loader)}\\n\\ttest: {len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.695481Z",
     "start_time": "2025-03-14T17:35:49.691924900Z"
    }
   },
   "id": "c6bebd9eeed34711",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abdcc80122d23428"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1) \n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module):\n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data, annotations, mask, has_annotations in train_loader:\n",
    "        where = torch.where(has_annotations[0] == 1)\n",
    "        ridxs = torch.randperm(where[0].shape[0])[:100]\n",
    "        spectral_sample.append(data[:, :, where[0][ridxs],where[1][ridxs]].permute(0,2,1).flatten(0,1).numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.702127300Z",
     "start_time": "2025-03-14T17:35:49.698570200Z"
    }
   },
   "id": "e49ecda26ba39e97",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f550708e9f4c74c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class custompad(nn.Module):\n",
    "    def __init__(self,padvec):\n",
    "        super().__init__()\n",
    "        self.padvec = padvec\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return torch.nn.functional.pad(x,self.padvec)\n",
    "    \n",
    "class quicknet_3(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim,n_classes,dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input processing and dimensionality reduction\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim)\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim,reduce_dim)\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(reduce_dim, 512, 3, stride=1, padding=1, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(512, 512, 1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(512, 512, 1, stride=1, padding=0)\n",
    "        \n",
    "        # Normalisation Layers\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Fc Layers\n",
    "        self.fc1 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        \n",
    "        # Additional kit\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.activation,\n",
    "            self.bn1,\n",
    "            self.conv2,\n",
    "            self.activation,\n",
    "            self.bn2,\n",
    "            self.conv3,\n",
    "            self.activation,\n",
    "            self.bn3,  \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.activation,\n",
    "            self.bn4,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.input_processing(x)\n",
    "        features = self.feature_extractor(inputs)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    " \n",
    "class quicknet_101(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim,n_classes,dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input processing and dimensionality reduction\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim)\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim,reduce_dim)\n",
    "   \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(reduce_dim, 32, 5, stride=1, dilation=1, padding=0, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, dilation=4, stride=1, padding=0, padding_mode='reflect')\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, dilation=8, stride=1, padding=0, padding_mode='reflect')\n",
    "        \n",
    "        # Normalisation Layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Conv2d(2304,256,1,stride=1,padding=0)\n",
    "        \n",
    "        # Additional Kit\n",
    "        self.unfold = nn.Unfold((6,6), dilation=16, padding=(0,0), stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,dilation=2,stride=1,padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,dilation=4,stride=1,padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,dilation=8,stride=1,padding=0)\n",
    "        self.activation = nn.GELU()\n",
    "        self.prepad = custompad((61,61,61,61,0,0,0,0))\n",
    "    \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.activation,\n",
    "            self.pool1,\n",
    "            self.bn1,\n",
    "            self.conv2,\n",
    "            self.activation,\n",
    "            self.pool2,\n",
    "            self.bn2,\n",
    "            self.conv3,\n",
    "            self.activation,\n",
    "            self.pool3,\n",
    "            self.bn3,\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.activation,\n",
    "            self.bn4,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h,w = x.shape[-2:]\n",
    "        inputs = self.input_processing(x)\n",
    "        features = self.feature_extractor(self.prepad(inputs))\n",
    "        logits = self.classifier(self.unfold(features).reshape(-1,64*36,h,w))\n",
    "        return logits\n",
    "    \n",
    "class quicknet_fusion(nn.Module):\n",
    "    def __init__(self,model1,model3,n_classes):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model3 = model3\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256*2,256,1,stride=1,padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256,n_classes,1,stride=1,padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.model1(x)\n",
    "        x3 = self.model3(x)\n",
    "        out = self.classifier(torch.cat([x1,x3],dim=1))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.741923500Z",
     "start_time": "2025-03-14T17:35:49.702127300Z"
    }
   },
   "id": "33535f38f1458959",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicknet model with 1.914M params\n"
     ]
    }
   ],
   "source": [
    "model3 = quicknet_3(\n",
    "    input_dim=len(wavenumbers_used),\n",
    "    reduce_dim=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,\n",
    "    n_classes=n_classes,\n",
    "    dropout_p=dropout_p)\n",
    "model101 = quicknet_101(\n",
    "    input_dim=len(wavenumbers_used),\n",
    "    reduce_dim=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,\n",
    "    n_classes=n_classes,\n",
    "    dropout_p=dropout_p)\n",
    "model = quicknet_fusion(model3,model101,n_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"quicknet model with {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M params\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.743987100Z",
     "start_time": "2025-03-14T17:35:49.714895Z"
    }
   },
   "id": "ed359ba87a31e556",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dfdaf2ad556fa46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=15, threshold=0.01, cooldown=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.743987100Z",
     "start_time": "2025-03-14T17:35:49.727802900Z"
    }
   },
   "id": "50a59f2ddf1e3f0d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_losses,validation_losses = [],[]\n",
    "training_accs,validation_accs = [],[]\n",
    "training_f1ms,validation_f1ms = [],[]\n",
    "training_f1s,validation_f1s = [],[]\n",
    "lr_decreases = []\n",
    "current_iters = 0\n",
    "best_val_f1 = 0\n",
    "best_val_iter = 0\n",
    "stop_training=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:35:49.744987200Z",
     "start_time": "2025-03-14T17:35:49.731222Z"
    }
   },
   "id": "51f82808428ba870",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ✰ ✰ ✰ EPOCH 1 ✰ ✰ ✰ \n",
      "train : █\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\n ✰ ✰ ✰ EPOCH {epoch+1} ✰ ✰ ✰ \")\n",
    "    \n",
    "    # reset running metrics\n",
    "    running_loss_train, running_loss_val = 0, 0\n",
    "    train_preds,train_targets = [],[]\n",
    "    val_preds,val_targets = [],[]\n",
    "    \n",
    "    # Train loop\n",
    "    model.train()\n",
    "    batch_frac = 42 / (len(train_loader))\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(train_loader):\n",
    "        print(f\"train : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "        \n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_train += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        train_preds.extend(preds.detach().cpu().numpy())\n",
    "        train_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"train : {'█'*42}\")\n",
    "        \n",
    "    # Validate loop\n",
    "    model.eval()\n",
    "    batch_frac = 42 / len(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, annot, mask, has_annotations) in enumerate(val_loader):\n",
    "            print(f\"val   : {'█'*int(batch_idx*batch_frac)}\", end=\"\\r\")\n",
    "            \n",
    "            # Put data and label on device\n",
    "            data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "            \n",
    "            # Push data through model\n",
    "            out = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "            loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "            \n",
    "            # Calculate metrics\n",
    "            running_loss_val += loss.cpu().item()\n",
    "            targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "            preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "            val_preds.extend(preds.detach().cpu().numpy())\n",
    "            val_targets.extend(targets.detach().cpu().numpy())\n",
    "    print(f\"val   : {'█'*42}\")\n",
    "    \n",
    "    # calculate epoch metrics for train set\n",
    "    train_acc = accuracy_score(train_targets, train_preds); training_accs.append(train_acc)\n",
    "    train_f1m = f1_score(train_targets, train_preds, average='macro'); training_f1ms.append(train_f1m)\n",
    "    train_f1 = f1_score(train_targets, train_preds, average=None); training_f1s.append(train_f1)\n",
    "    train_loss = running_loss_train / (len(dataset_train)); training_losses.append(train_loss)\n",
    "    \n",
    "    # calculate epoch metrics for val set\n",
    "    val_acc = accuracy_score(val_targets, val_preds); validation_accs.append(val_acc)\n",
    "    val_f1m = f1_score(val_targets, val_preds, average='macro'); validation_f1ms.append(val_f1m)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average=None); validation_f1s.append(val_f1)\n",
    "    val_loss = running_loss_val / (len(dataset_val)); validation_losses.append(val_loss)\n",
    "    \n",
    "    # Update\n",
    "    print(f\"TRAIN --- | Loss: {train_loss:.4} | OA: {train_acc:.4} | f1: {train_f1m:.4}\")\n",
    "    print(f\"VAL ----- | Loss: {val_loss:.4} | OA: {val_acc:.4} | f1: {val_f1m:.4}\")\n",
    "    \n",
    "    scheduler.step(val_f1m)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != lr:\n",
    "        print(f\"Val f1 plateaued, lr {lr} -> {new_lr}\")\n",
    "        lr = new_lr\n",
    "        lr_decreases.append(epoch)\n",
    "        if len(lr_decreases) >= 3: \n",
    "            print(\"Val f1 decreased thrice, ending training early\")\n",
    "            break\n",
    "\n",
    "    if val_f1m > best_val_f1:\n",
    "        best_val_f1 = val_f1m\n",
    "        best_val_epoch = epoch\n",
    "        if not is_local:\n",
    "            torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')\n",
    "\n",
    "if not is_local:\n",
    "    model.load_state_dict(torch.load(rf'./model_weights_{seed}.pt', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:36:25.677476900Z",
     "start_time": "2025-03-14T17:35:49.733594600Z"
    }
   },
   "id": "7c6adbd14fd799f6",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307cf693fb06cb7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test\n",
    "running_loss_test = 0\n",
    "test_preds, test_targets = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, annot, mask, has_annotations) in enumerate(test_loader):\n",
    "        # Put data and label on device\n",
    "        data = data.to(device); annot = annot.to(device); has_annotations = has_annotations.to(device)\n",
    "        \n",
    "        # Push data through model\n",
    "        out = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(out,annot.argmax(dim=1)) * has_annotations # loss per pixel\n",
    "        loss = loss.sum() / (has_annotations.sum()) # mean loss per annotated pixel\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss_test += loss.cpu().item()\n",
    "        targets = annot.argmax(dim=1)[has_annotations] # class targets on annotated pixels\n",
    "        preds = out.argmax(dim=1)[has_annotations] # predicted values on annotated pixels\n",
    "        test_preds.extend(preds.detach().cpu().numpy())\n",
    "        test_targets.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "# calculate test set metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_f1m = f1_score(test_targets, test_preds, average='macro')\n",
    "test_f1 = f1_score(test_targets, test_preds, average=None)\n",
    "test_loss = running_loss_test / batch_idx\n",
    "\n",
    "print(f\"TEST ---- | Loss: {test_loss:.4} | OA: {test_acc:.4} | f1: {test_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(test_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:36:25.679550Z",
     "start_time": "2025-03-14T17:36:25.678477200Z"
    }
   },
   "id": "242a34c1a6fe3be8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff17a2ebecf4ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "ax[0].plot(np.arange(1,len(training_losses)+1),np.array(training_losses),color='cornflowerblue',label=\"train\")\n",
    "ax[0].plot(np.arange(1,len(validation_losses)+1),np.array(validation_losses),color='orange',label=\"validation\")\n",
    "ax[0].scatter(len(validation_losses),test_loss,color='green',label=\"test\",marker=\"x\")\n",
    "ax[0].set_title(\"loss curves\"); ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(1,len(training_accs)+1),np.array(training_accs),color='cornflowerblue',label=\"train\")\n",
    "ax[1].plot(np.arange(1,len(validation_accs)+1),np.array(validation_accs),color='orange',label=\"validation\")\n",
    "ax[1].scatter(len(validation_losses),test_acc,color='green',label=\"test\",marker=\"x\")\n",
    "ax[1].set_title(\"accuracy\"); ax[1].legend()\n",
    "\n",
    "ax[2].plot(np.arange(1,len(training_f1ms)+1),np.array(training_f1ms),color='cornflowerblue',label=\"train\")\n",
    "ax[2].plot(np.arange(1,len(validation_f1ms)+1),np.array(validation_f1ms),color='orange',label=\"validation\")\n",
    "ax[2].scatter(len(validation_losses),test_f1m,color='green',label=\"test\",marker=\"x\")\n",
    "ax[2].set_title(\"macro f1\"); ax[2].legend()\n",
    "\n",
    "for lrd in lr_decreases:\n",
    "    ax[0].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[1].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[2].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "\n",
    "ax[0].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[1].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "ax[2].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-14T17:36:25.678513800Z"
    }
   },
   "id": "5aa6f4abb81d43a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(15,5)); ax = ax.flatten()\n",
    "for cls in range(6):\n",
    "    ax[cls].plot(np.arange(1,len(training_f1s)+1),[i[cls] for i in training_f1s], color='black', label=\"train\")\n",
    "    ax[cls].plot(np.arange(1,len(validation_f1s)+1),[i[cls] for i in validation_f1s], color=annotation_class_colors[cls]/255, label=\"val\")\n",
    "    ax[cls].set_title(f\"{annotation_class_names[cls]}\")\n",
    "    ax[cls].legend()\n",
    "    ax[cls].scatter(len(validation_losses),test_f1[cls],color='green',label=\"test\",marker=\"x\")\n",
    "    for lrd in lr_decreases:\n",
    "        ax[cls].axvline(x=lrd, ymin=0, ymax=1, color='grey')\n",
    "    ax[cls].axvline(x=best_val_epoch, ymin=0, ymax=1, color='red',alpha=0.3)\n",
    "fig.suptitle(\"Class-specific F1 scores\")\n",
    "plt.tight_layout()\n",
    "if not is_local:\n",
    "    plt.savefig(f'./loss_curve_by_class_{seed}.png')\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:36:25.681631Z",
     "start_time": "2025-03-14T17:36:25.680594200Z"
    }
   },
   "id": "60c4c1d2914ee636",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish experiment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd7e4042615b52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not is_local:\n",
    "    model = model.cpu()\n",
    "    torch.save(model.state_dict(), rf'./model_weights_{seed}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-14T17:36:25.680594200Z"
    }
   },
   "id": "b8248f10250d5d36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read existing results file\n",
    "if not is_local:\n",
    "    if os.path.isfile('results.txt'):\n",
    "        f = open('results.txt','r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    else: \n",
    "        lines = [x+', \\n' for x in['seed',*annotation_class_names,'overall_acc','macro_f1']]\n",
    "        \n",
    "    # Process files\n",
    "    lines[0] = lines[0].replace('\\n',str(seed) + ', \\n')\n",
    "    for cls in range(n_classes):\n",
    "        lines[cls+1] = lines[cls+1].replace('\\n',str(test_f1[cls]) + ', \\n' )\n",
    "    lines[n_classes+1] = lines[n_classes+1].replace('\\n',str(test_acc) + ', \\n')\n",
    "    lines[n_classes+2] = lines[n_classes+2].replace('\\n',str(test_f1m) + ', \\n')\n",
    "    \n",
    "    f = open('results.txt','w')\n",
    "    f.write(''.join(lines))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-14T17:36:25.683692600Z",
     "start_time": "2025-03-14T17:36:25.681631Z"
    }
   },
   "id": "bcb3c45e041764e8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
