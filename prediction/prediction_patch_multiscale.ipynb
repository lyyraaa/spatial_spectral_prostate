{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:52:36.194786900Z",
     "start_time": "2025-03-07T09:52:30.791139200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78042e76952fecbc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ellipsis, slice(0, 965, None))\n"
     ]
    }
   ],
   "source": [
    "is_local = True # todo\n",
    "\n",
    "# Experiment\n",
    "seed = 3 if is_local else int(sys.argv[-2])\n",
    "torch.manual_seed(seed)\n",
    "image_size = 256\n",
    "\n",
    "# Data: which wavenumbers are even allowed to be considered?\n",
    "wv_start = 0\n",
    "wv_end = 965\n",
    "\n",
    "# Data loading\n",
    "test_set_fraction = 0.2\n",
    "val_set_fraction = 0.2\n",
    "batch_size= 64\n",
    "patch_dim = 101\n",
    "use_augmentation = True\n",
    "\n",
    "# Network\n",
    "dropout_p=0.5\n",
    "\n",
    "# Training schedule\n",
    "lr = 1e-5\n",
    "l2 = 5e-1\n",
    "max_iters = 5000\n",
    "pseudo_epoch = 100\n",
    "\n",
    "# dimensionality reduction parameters\n",
    "r_method = 'linear' # {'linear','pca,'fixed'} # todo change to linear\n",
    "reduce_dim = 64 if is_local else int(sys.argv[-1]) # used only for r_method = 'pca' or 'linear'\n",
    "channels_used = np.s_[...,wv_start:wv_end] # used only when r_method = 'fixed'\n",
    "print(channels_used)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:30.083452900Z",
     "start_time": "2025-03-07T09:58:30.056826700Z"
    }
   },
   "id": "33d01130ebec9bd2",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 cores\n",
      "Using 965/965 wavenumbers\n"
     ]
    }
   ],
   "source": [
    "def csf_fp(filepath):\n",
    "    return filepath.replace('D:/datasets','D:/datasets' if is_local else './')\n",
    "\n",
    "master = pd.read_excel(csf_fp(rf'D:/datasets/pcuk2023_ftir_whole_core/master_sheet.xlsx'))\n",
    "slide = master['slide'].to_numpy()\n",
    "patient_id = master['patient_id'].to_numpy()\n",
    "hdf5_filepaths = np.array([csf_fp(fp) for fp in master['hdf5_filepath']])\n",
    "annotation_filepaths = np.array([csf_fp(fp) for fp in master['annotation_filepath']])\n",
    "mask_filepaths = np.array([csf_fp(fp) for fp in master['mask_filepath']])\n",
    "wavenumbers = np.load(csf_fp(f'D:/datasets/pcuk2023_ftir_whole_core/wavenumbers.npy'))[wv_start:wv_end]\n",
    "wavenumbers_used = wavenumbers[channels_used]\n",
    "\n",
    "annotation_class_colors = np.array([[0,255,0],[128,0,128],[255,0,255],[0,0,255],[255,165,0],[255,0,0]])\n",
    "annotation_class_names = np.array(['epithelium_n','stroma_n','epithelium_c','stroma_c','corpora_amylacea','blood'])\n",
    "n_classes = len(annotation_class_names)\n",
    "print(f\"Loaded {len(slide)} cores\")\n",
    "print(f\"Using {len(wavenumbers_used)}/{len(wavenumbers)} wavenumbers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:52:37.154333Z",
     "start_time": "2025-03-07T09:52:36.256817800Z"
    }
   },
   "id": "226cc4e9b39bdf59",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define datasets and loaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8185c5993829f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients per data split:\n",
      "\tTRAIN: 139\n",
      "\tVAL: 48\n",
      "\tTEST: 41\n"
     ]
    }
   ],
   "source": [
    "unique_pids = np.unique(patient_id)\n",
    "pids_trainval, pids_test, _, _ = train_test_split(\n",
    "    unique_pids, np.zeros_like(unique_pids), test_size=test_set_fraction, random_state=seed)\n",
    "pids_train, pids_val, _, _ = train_test_split(\n",
    "    pids_trainval, np.zeros_like(pids_trainval), test_size=(val_set_fraction/(1-test_set_fraction)), random_state=seed)\n",
    "where_train = np.where(np.isin(patient_id,pids_train))\n",
    "where_val = np.where(np.isin(patient_id,pids_val))\n",
    "where_test = np.where(np.isin(patient_id,pids_test))\n",
    "print(f\"Patients per data split:\\n\\tTRAIN: {len(where_train[0])}\\n\\tVAL: {len(where_val[0])}\\n\\tTEST: {len(where_test[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:52:37.166390900Z",
     "start_time": "2025-03-07T09:52:37.155329400Z"
    }
   },
   "id": "7c0289515237ade8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ftir_annot_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 hdf5_filepaths, mask_filepaths, annotation_filepaths, channels_use):\n",
    "        self.hdf5_filepaths = hdf5_filepaths\n",
    "        self.mask_filepaths = mask_filepaths\n",
    "        self.annotation_filepaths = annotation_filepaths\n",
    "        self.channels_use = channels_use\n",
    "        \n",
    "        # class data\n",
    "        self.annotation_class_colors = annotation_class_colors\n",
    "        self.annotation_class_names = annotation_class_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hdf5_filepaths)\n",
    "    \n",
    "    # split annotations from H x W x 3 to C x H x W, one/zerohot along C dimension\n",
    "    def split_annotations(self,annotations_img):\n",
    "        split = torch.zeros((len(self.annotation_class_colors),*annotations_img.shape[:-1]))\n",
    "        for c,col in enumerate(self.annotation_class_colors):\n",
    "            split[c,:,:] = torch.from_numpy(np.all(annotations_img == self.annotation_class_colors[c],axis=-1)) \n",
    "        return split\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        # open hdf5 file\n",
    "        hdf5_file = h5py.File(self.hdf5_filepaths[idx],'r')\n",
    "        \n",
    "        # get mask\n",
    "        mask = torch.from_numpy(\n",
    "            hdf5_file['mask'][:],\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # get ftir\n",
    "        ftir = torch.from_numpy(\n",
    "            hdf5_file['spectra'][*self.channels_use],\n",
    "        ).permute(2,0,1)\n",
    "        hdf5_file.close()\n",
    "        ftir *= mask\n",
    "        \n",
    "        # get annotations\n",
    "        annotations = self.split_annotations(cv2.imread(self.annotation_filepaths[idx])[:,:,::-1])\n",
    "        annotations *= mask\n",
    "        \n",
    "        \n",
    "        return ftir, annotations, mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:33.332503800Z",
     "start_time": "2025-03-07T09:58:33.325110200Z"
    }
   },
   "id": "11a31b3dd7d2a6c5",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_train = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_train], mask_filepaths[where_train], annotation_filepaths[where_train], channels_used,\n",
    ")\n",
    "dataset_val = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_val], mask_filepaths[where_val], annotation_filepaths[where_val], channels_used,\n",
    ")\n",
    "dataset_test = ftir_annot_dataset(\n",
    "    hdf5_filepaths[where_test], mask_filepaths[where_test], annotation_filepaths[where_test], channels_used,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:33.675725200Z",
     "start_time": "2025-03-07T09:58:33.668143300Z"
    }
   },
   "id": "5e60cdfff125108f",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dimensionality reduction methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c5f71f1f034ae7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearReduction(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.projection = nn.Conv2d(input_dim,reduce_dim,kernel_size=1,stride=1) \n",
    "        self.projection_norm = nn.BatchNorm2d(reduce_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.projection_norm(self.projection(self.input_norm(x)))\n",
    "    \n",
    "class PCAReduce(nn.Module): \n",
    "    def __init__(self,reduce_dim,means,loadings):\n",
    "        super().__init__()\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.register_buffer('means', torch.from_numpy(means).float().reshape(1,-1,1,1))\n",
    "        self.register_buffer('loadings', torch.from_numpy(loadings).float())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        projected = x - self.means\n",
    "        \n",
    "        b,c,h,w = projected.shape\n",
    "        projected = projected.permute(0,2,3,1).reshape(b,h*w,c)\n",
    "        projected = torch.matmul(projected, self.loadings.T)\n",
    "        projected = projected.reshape(b,h,w,self.reduce_dim).permute(0,3,1,2)\n",
    "        \n",
    "        return projected\n",
    "        \n",
    "class FixedReduction(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.input_norm(x)\n",
    "\n",
    "if r_method == 'pca':\n",
    "    spectral_sample = []\n",
    "    batch_samples = 0\n",
    "    for data,label in train_loader:\n",
    "        spectral_sample.append(data[...,patch_dim//2,patch_dim//2].numpy())\n",
    "        batch_samples += 1\n",
    "        if batch_samples > 10000//batch_size: break\n",
    "    spectral_sample = np.concatenate(spectral_sample,axis=0)\n",
    "    spectral_means = np.mean(spectral_sample,axis=0)\n",
    "    spectral_sample -= spectral_means\n",
    "    pca = PCA(n_components=reduce_dim)\n",
    "    pca.fit(spectral_sample)\n",
    "    spectral_loadings = pca.components_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:34.223651300Z",
     "start_time": "2025-03-07T09:58:34.215538700Z"
    }
   },
   "id": "a5d18c5ca1820c0c",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ef87bd19226994d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class patch101_cnn(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim,n_classes,dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input processing and dimensionality reduction\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim)\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim,reduce_dim)\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(reduce_dim, 32, 5, stride=2, padding=0, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, padding_mode='reflect')\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        # Normalisation Layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Fc Layers\n",
    "        self.fc1 = nn.Linear(2304, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        # Additional kit\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.bn1,\n",
    "            self.conv2,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.bn2,\n",
    "            self.conv3,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.bn3,  \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.activation,\n",
    "            self.bn4,\n",
    "            self.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.input_processing(x)\n",
    "        features = self.feature_extractor(inputs)\n",
    "        logits = self.classifier(features.flatten(1))\n",
    "        return logits\n",
    "    \n",
    "class patch3_cnn(nn.Module):\n",
    "    def __init__(self,input_dim,reduce_dim,n_classes,dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # reduction\n",
    "        # input processing and dimensionality reduction\n",
    "        if r_method == 'pca':\n",
    "            self.input_processing = PCAReduce(reduce_dim,spectral_means,spectral_loadings)\n",
    "        elif r_method == 'fixed':\n",
    "            self.input_processing = FixedReduction(input_dim)\n",
    "        elif r_method == 'linear':\n",
    "            self.input_processing = LinearReduction(input_dim,reduce_dim)\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(reduce_dim, 512, 3, stride=1, padding=0, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(512, 512, 1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(512, 512, 1, stride=1, padding=0)\n",
    "        \n",
    "        # Normalisation Layers\n",
    "        self.input_norm = nn.BatchNorm2d(input_dim)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Fc Layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        # Additional kit\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.activation,\n",
    "            self.bn1,\n",
    "            self.conv2,\n",
    "            self.activation,\n",
    "            self.bn2,\n",
    "            self.conv3,\n",
    "            self.activation,\n",
    "            self.bn3,  \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.activation,\n",
    "            self.bn4,\n",
    "            self.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.input_processing(x)\n",
    "        features = self.feature_extractor(inputs)\n",
    "        logits = self.classifier(features.flatten(1))\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:34.572219700Z",
     "start_time": "2025-03-07T09:58:34.561336600Z"
    }
   },
   "id": "57e7547640dfddda",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class fusion_model(nn.Module):\n",
    "    def __init__(self,model1,model3,n_classes):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model3 = model3\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*2,256),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.model1(x[:,:,49:-49,49:-49])\n",
    "        x3 = self.model3(x)\n",
    "        out = self.classifier(torch.cat([x1,x3],dim=1))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:34.779117300Z",
     "start_time": "2025-03-07T09:58:34.774628200Z"
    }
   },
   "id": "23e13202842d5236",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_model with 1.919M params composed of:\n",
      "\tpatch3_model with 1.023M params\n",
      "\tpatch101_model with 0.763M params\n"
     ]
    }
   ],
   "source": [
    "model3 = patch3_cnn(\n",
    "    input_dim=len(wavenumbers_used),\n",
    "    reduce_dim=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,\n",
    "    n_classes=n_classes,\n",
    "    dropout_p=dropout_p)\n",
    "model101 = patch101_cnn(\n",
    "    input_dim=len(wavenumbers_used),\n",
    "    reduce_dim=len(wavenumbers_used) if r_method == 'fixed' else reduce_dim,\n",
    "    n_classes=n_classes,\n",
    "    dropout_p=dropout_p)\n",
    "model = fusion_model(model3,model101,n_classes)\n",
    "\n",
    "print(f\"fusion_model with {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M params composed of:\")\n",
    "print(f\"\\tpatch3_model with {sum(p.numel() for p in model3.parameters() if p.requires_grad) / 1e6:.3f}M params\")\n",
    "print(f\"\\tpatch101_model with {sum(p.numel() for p in model101.parameters() if p.requires_grad) / 1e6:.3f}M params\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:35.113014900Z",
     "start_time": "2025-03-07T09:58:35.095213200Z"
    }
   },
   "id": "f042c4e2b5ac3d46",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fae7ad9f49e9083"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_weights = torch.load(\"../models/patch_multiscale_linear64_seed3.pt\", weights_only=True) # todo remove this branch\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(model_weights,strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:36.052845900Z",
     "start_time": "2025-03-07T09:58:36.030813700Z"
    }
   },
   "id": "427fd801c695aa2",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loop through datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89df5143bc6fc160"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a105f953042d865e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "time_total = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T09:58:47.323309600Z",
     "start_time": "2025-03-07T09:58:47.317030100Z"
    }
   },
   "id": "422be564f022f0c8",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/139\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m r_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(unfolded\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m256\u001B[39m,\u001B[38;5;241m64\u001B[39m):\n\u001B[1;32m---> 30\u001B[0m             out[c_idx,:,r_idx,col:col\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m64\u001B[39m] \u001B[38;5;241m=\u001B[39m model(unfolded[c_idx,r_idx,col:col\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m64\u001B[39m])\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     32\u001B[0m targets \u001B[38;5;241m=\u001B[39m annot\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[has_annot]\n\u001B[0;32m     33\u001B[0m preds \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[has_annot]\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[19], line 16\u001B[0m, in \u001B[0;36mfusion_model.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[0;32m     15\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel1(x[:,:,\u001B[38;5;241m49\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m49\u001B[39m,\u001B[38;5;241m49\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m49\u001B[39m])\n\u001B[1;32m---> 16\u001B[0m     x3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel3(x)\n\u001B[0;32m     17\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(torch\u001B[38;5;241m.\u001B[39mcat([x1,x3],dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[18], line 57\u001B[0m, in \u001B[0;36mpatch101_cnn.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 57\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_processing(x)\n\u001B[0;32m     58\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_extractor(inputs)\n\u001B[0;32m     59\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(features\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[17], line 10\u001B[0m, in \u001B[0;36mLinearReduction.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprojection_norm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprojection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_norm(x)))\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\phd-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    548\u001B[0m     )\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[0;32m    551\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "set_preds,set_targets = [], []\n",
    "pred_images, annot_images = [], []\n",
    "loader_use = train_loader\n",
    "start_t = time.time()\n",
    "with torch.no_grad():\n",
    "    for bidx, (data, annot, mask) in enumerate(loader_use):\n",
    "        print(f\"{bidx}/{len(loader_use)}\",end='\\r')\n",
    "        data = data.to(device); annot = annot.to(device); mask = mask.to(device)\n",
    "        has_annot = annot.sum(dim=1) != 0\n",
    "        \n",
    "        out = torch.zeros((data.shape[0],n_classes,data.shape[2],data.shape[3]))\n",
    "        pad_data = torch.nn.functional.pad(data,(patch_dim//2,patch_dim//2,patch_dim//2,patch_dim//2,0,0,0,0))\n",
    "        \n",
    "        unfolded = pad_data.unfold(\n",
    "            2,(101),1\n",
    "        ).unfold(\n",
    "            3,(101),1\n",
    "        ).permute(\n",
    "            0,2,3,1,4,5\n",
    "        ).view(\n",
    "            data.shape[0],\n",
    "            image_size,image_size,\n",
    "            len(wavenumbers_used),\n",
    "            patch_dim,\n",
    "            patch_dim,\n",
    "        )\n",
    "        for c_idx in range(unfolded.shape[0]):\n",
    "            for r_idx in range(unfolded.shape[1]):\n",
    "                for col in range(0,256,64):\n",
    "                    out[c_idx,:,r_idx,col:col+64] = model(unfolded[c_idx,r_idx,col:col+64]).permute(1,0)\n",
    "        \n",
    "        targets = annot.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        preds = out.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        set_targets.extend(targets.detach().cpu().numpy())\n",
    "        set_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        bg = data[:,229,:,:].clone()\n",
    "        for b in range(bg.shape[0]): bg[b] -= bg[b].min()\n",
    "        for b in range(bg.shape[0]): bg[b] /= bg[b].max()\n",
    "        bg = torch.stack([bg,bg,bg],dim=-1).detach().cpu().numpy()\n",
    "        \n",
    "        # Prediction images\n",
    "        pred_image = annotation_class_colors[out.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        pred_image *= mask.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        pred_images.extend(pred_image)\n",
    "        annot_image = annotation_class_colors[annot.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        annot_image *= has_annot.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        annot_image = np.where(has_annot.unsqueeze(-1).repeat(1,1,1,1,3).detach().cpu().numpy(), annot_image, bg)[0]\n",
    "        annot_images.extend(annot_image)\n",
    "time_total += time.time() - start_t \n",
    "\n",
    "# calculate test set metrics\n",
    "set_acc = accuracy_score(set_targets, set_preds)\n",
    "set_f1m = f1_score(set_targets, set_preds, average='macro')\n",
    "set_f1 = f1_score(set_targets, set_preds, average=None)\n",
    "\n",
    "print(f\"DATASET TRAIN --- | OA: {set_acc:.4} | f1: {set_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(set_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T10:11:27.874232500Z",
     "start_time": "2025-03-07T10:03:00.134455700Z"
    }
   },
   "id": "9436048c74601322",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    i = 0\n",
    "    for i in range(0,len(pred_images),2):\n",
    "        fig,ax = plt.subplots(1,2,figsize=(16.5,16.5/4)); ax = ax.flatten()\n",
    "        ax[0].matshow(np.hstack([pred_images[i],annot_images[i]])); ax[0].set_axis_off()\n",
    "        ax[1].matshow(np.hstack([pred_images[i+1],annot_images[i+1]])); ax[1].set_axis_off()\n",
    "        fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T10:01:53.943575100Z",
     "start_time": "2025-03-07T10:01:53.938586600Z"
    }
   },
   "id": "76fa534ee9d2b87",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76cf080ccc8a92cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "set_preds,set_targets = [], []\n",
    "pred_images, annot_images = [], []\n",
    "loader_use = val_loader\n",
    "start_t = time.time()\n",
    "with torch.no_grad():\n",
    "    for bidx, (data, annot, mask) in enumerate(loader_use):\n",
    "        print(f\"{bidx}/{len(loader_use)}\",end='\\r')\n",
    "        data = data.to(device); annot = annot.to(device); mask = mask.to(device)\n",
    "        has_annot = annot.sum(dim=1) != 0\n",
    "        \n",
    "        out = torch.zeros((data.shape[0],n_classes,data.shape[2],data.shape[3]))\n",
    "        pad_data = torch.nn.functional.pad(data,(patch_dim//2,patch_dim//2,patch_dim//2,patch_dim//2,0,0,0,0))\n",
    "        \n",
    "        unfolded = pad_data.unfold(\n",
    "            2,(101),1\n",
    "        ).unfold(\n",
    "            3,(101),1\n",
    "        ).permute(\n",
    "            0,2,3,1,4,5\n",
    "        ).view(\n",
    "            data.shape[0],\n",
    "            image_size,image_size,\n",
    "            len(wavenumbers_used),\n",
    "            patch_dim,\n",
    "            patch_dim,\n",
    "        )\n",
    "        for c_idx in range(unfolded.shape[0]):\n",
    "            for r_idx in range(unfolded.shape[1]):\n",
    "                for col in range(0,256,64):\n",
    "                    out[c_idx,:,r_idx,col:col+64] = model(unfolded[c_idx,r_idx,col:col+64]).permute(1,0)\n",
    "        \n",
    "        targets = annot.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        preds = out.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        set_targets.extend(targets.detach().cpu().numpy())\n",
    "        set_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        bg = data[:,229,:,:].clone()\n",
    "        for b in range(bg.shape[0]): bg[b] -= bg[b].min()\n",
    "        for b in range(bg.shape[0]): bg[b] /= bg[b].max()\n",
    "        bg = torch.stack([bg,bg,bg],dim=-1).detach().cpu().numpy()\n",
    "        \n",
    "        # Prediction images\n",
    "        pred_image = annotation_class_colors[out.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        pred_image *= mask.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        pred_images.extend(pred_image)\n",
    "        annot_image = annotation_class_colors[annot.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        annot_image *= has_annot.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        annot_image = np.where(has_annot.unsqueeze(-1).repeat(1,1,1,1,3).detach().cpu().numpy(), annot_image, bg)[0]\n",
    "        annot_images.extend(annot_image)\n",
    "time_total += time.time() - start_t \n",
    "        \n",
    "# calculate test set metrics\n",
    "set_acc = accuracy_score(set_targets, set_preds)\n",
    "set_f1m = f1_score(set_targets, set_preds, average='macro')\n",
    "set_f1 = f1_score(set_targets, set_preds, average=None)\n",
    "\n",
    "print(f\"DATASET VAL ----- | OA: {set_acc:.4} | f1: {set_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(set_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-03T13:20:05.876115300Z"
    }
   },
   "id": "e4031b7c44365335",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    i = 0\n",
    "    for i in range(0,len(pred_images),2):\n",
    "        fig,ax = plt.subplots(1,2,figsize=(16.5,16.5/4)); ax = ax.flatten()\n",
    "        ax[0].matshow(np.hstack([pred_images[i],annot_images[i]])); ax[0].set_axis_off()\n",
    "        ax[1].matshow(np.hstack([pred_images[i+1],annot_images[i+1]])); ax[1].set_axis_off()\n",
    "        fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-03T13:20:05.876115300Z"
    }
   },
   "id": "317dcff30e684e5a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9edbc24f843a764"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "set_preds,set_targets = [], []\n",
    "pred_images, annot_images = [], []\n",
    "loader_use = test_loader\n",
    "start_t = time.time()\n",
    "with torch.no_grad():\n",
    "    for bidx, (data, annot, mask) in enumerate(loader_use):\n",
    "        print(f\"{bidx}/{len(loader_use)}\",end='\\r')\n",
    "        data = data.to(device); annot = annot.to(device); mask = mask.to(device)\n",
    "        has_annot = annot.sum(dim=1) != 0\n",
    "        \n",
    "        out = torch.zeros((data.shape[0],n_classes,data.shape[2],data.shape[3]))\n",
    "        pad_data = torch.nn.functional.pad(data,(patch_dim//2,patch_dim//2,patch_dim//2,patch_dim//2,0,0,0,0))\n",
    "        \n",
    "        unfolded = pad_data.unfold(\n",
    "            2,(101),1\n",
    "        ).unfold(\n",
    "            3,(101),1\n",
    "        ).permute(\n",
    "            0,2,3,1,4,5\n",
    "        ).view(\n",
    "            data.shape[0],\n",
    "            image_size,image_size,\n",
    "            len(wavenumbers_used),\n",
    "            patch_dim,\n",
    "            patch_dim,\n",
    "        )\n",
    "        for c_idx in range(unfolded.shape[0]):\n",
    "            for r_idx in range(unfolded.shape[1]):\n",
    "                for col in range(0,256,64):\n",
    "                    out[c_idx,:,r_idx,col:col+64] = model(unfolded[c_idx,r_idx,col:col+64]).permute(1,0)\n",
    "        \n",
    "        targets = annot.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        preds = out.argmax(dim=1)[has_annot.detach().cpu()]\n",
    "        set_targets.extend(targets.detach().cpu().numpy())\n",
    "        set_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        bg = data[:,229,:,:].clone()\n",
    "        for b in range(bg.shape[0]): bg[b] -= bg[b].min()\n",
    "        for b in range(bg.shape[0]): bg[b] /= bg[b].max()\n",
    "        bg = torch.stack([bg,bg,bg],dim=-1).detach().cpu().numpy()\n",
    "        \n",
    "        # Prediction images\n",
    "        pred_image = annotation_class_colors[out.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        pred_image *= mask.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        pred_images.extend(pred_image)\n",
    "        annot_image = annotation_class_colors[annot.argmax(1).detach().cpu().numpy()] / 255.0\n",
    "        annot_image *= has_annot.squeeze().unsqueeze(-1).cpu().numpy()\n",
    "        annot_image = np.where(has_annot.unsqueeze(-1).repeat(1,1,1,1,3).detach().cpu().numpy(), annot_image, bg)[0]\n",
    "        annot_images.extend(annot_image)\n",
    "time_total += time.time() - start_t \n",
    "        \n",
    "# calculate test set metrics\n",
    "set_acc = accuracy_score(set_targets, set_preds)\n",
    "set_f1m = f1_score(set_targets, set_preds, average='macro')\n",
    "set_f1 = f1_score(set_targets, set_preds, average=None)\n",
    "\n",
    "print(f\"DATASET TEST ---- | OA: {set_acc:.4} | f1: {set_f1m:.4}\")\n",
    "for cls_idx, f1 in enumerate(set_f1):\n",
    "    print(f\"{annotation_class_names[cls_idx]}{(20 - len(annotation_class_names[cls_idx])) * ' '} : {f1:.4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-03T13:20:05.876115300Z"
    }
   },
   "id": "a9ae29bc3b116cab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    i = 0\n",
    "    for i in range(0,len(pred_images),2):\n",
    "        fig,ax = plt.subplots(1,2,figsize=(16.5,16.5/4)); ax = ax.flatten()\n",
    "        ax[0].matshow(np.hstack([pred_images[i],annot_images[i]])); ax[0].set_axis_off()\n",
    "        ax[1].matshow(np.hstack([pred_images[i+1],annot_images[i+1]])); ax[1].set_axis_off()\n",
    "        fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T13:20:05.880248500Z",
     "start_time": "2025-03-03T13:20:05.878181300Z"
    }
   },
   "id": "f19459433fd5cfbb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"TIME TOTAL FOR ALL IMAGES: {time_total}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-03T13:20:05.880248500Z"
    }
   },
   "id": "6acd9aa8c4f6de55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
